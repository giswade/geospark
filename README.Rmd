---
title: "GeoSpark: R interface to GeoSpark"
output:
  github_document:
    fig_width: 9
    fig_height: 5
---

[![CRAN version](https://www.r-pkg.org/badges/version/geospark)](https://CRAN.R-project.org/package=geospark)
[![Build Status](https://travis-ci.org/harryprince/geospark.svg?branch=master)](https://travis-ci.org/harryprince/geospark)

## Introduction & Philosophy

The `geospark` R package aims at bringing local [sf](https://github.com/r-spatial/sf) functions to distributed spark mode with [GeoSpark](https://github.com/DataSystemsLab/GeoSpark) scala package.

Currently, `geospark` support most of important `sf` functions in spark, here is a [summary comparison](https://github.com/harryprince/geospark/blob/master/Reference.md).

## Installation

This package requires Apache Spark 2.X which you can install using `sparklyr::install_spark("2.4")`; in addition, you can install `geospark` as follows:

```{r eval = FALSE}
devtools::install_github("harryprince/geospark")
```

## Getting Started

In this example we will join spatial data using quadrad tree indexing. First, we will initialize the `geospark` extension and connecto to Spark using `sparklyr`:

```{r eval = FALSE}
library(sparklyr)
library(geospark)

sc <- spark_connect(master = "local", config = conf)
```

Next we will load some spatial dataset containing as polygons and points.

```{r eval = FALSE}
polygons <- read.table("inst/examples/polygons.txt", sep="|", col.names=c("area","geom"))
points <- read.table("inst/examples/points.txt", sep="|", col.names=c("city","state","geom"))

polygons_wkt <- copy_to(sc, polygons)
points_wkt <- copy_to(sc, points)
```

Now we can perform a GeoSpatial join using the `st_contains` which converts `wkt` into geometry object with 4326 `crs` which means a `wgs84` projection. To get the original data from `wkt` format, we will use the `st_geomfromwkt` functions. We can execute this spatial query using `DBI` as follows:

```{r eval = FALSE}
DBI::dbGetQuery(sc, "
  SELECT area, state, count(*) cnt FROM
    (SELECT area, ST_GeomFromWKT(polygons.geom ,'4326') as y FROM polygons) polygons
  INNER JOIN
    (SELECT ST_GeomFromWKT (points.geom,'4326') as x, state, city FROM points) points
  WHERE ST_Contains(polygons.y,points.x) GROUP BY area, state")
```
```
             area state cnt
1      texas area    TX  10
2     dakota area    SD   1
3     dakota area    ND  10
4 california area    CA  10
5   new york area    NY   9
```

Finally, we can disconnect:

```{r eval = FALSE}
spark_disconnect_all()
```

## Performance

### Configuration

To improve performance, it is recommended to use the `KryoSerializer` and the `GeoSparkKryoRegistrator` before connecting as follows:

```r
conf <- spark_config()
conf$spark.serializer <- "org.apache.spark.serializer.KryoSerializer"
conf$spark.kryo.registrator <- "org.datasyslab.geospark.serde.GeoSparkKryoRegistrator"
```

### Benchmarks

This performance comparison is an extract from the original [GeoSpark: A Cluster Computing Framework for
Processing Spatial Data](https://pdfs.semanticscholar.org/347d/992ceec645a28f4e7e45e9ab902cd75ecd92.pdf) paper:

No. |test case | the number of records
---|---|---
1|SELECT IDCODE FROM zhenlongxiang WHERE ST_Disjoint(geom,ST_GeomFromText(‘POLYGON((517000 1520000,619000 1520000,619000 2530000,517000 2530000,517000 1520000))’));|85,236 rows
2| SELECT fid FROM cyclonepoint WHERE ST_Disjoint(geom,ST_GeomFromText(‘POLYGON((90 3,170 3,170 55,90 55,90 3))’,4326)) | 60,591 rows

Query performance(ms),

No. | PostGIS/PostgreSQL |GeoSpark SQL| ESRI Spatial Framework for Hadoop
---|---|---|---
1 | 9631 | 480 |40,784
2 | 110872 |394| 64,217

According to this papaer, the Geospark SQL definitely outperforms PG and ESRI UDF under a very large data set.

If you are wondering how the spatial index accelerate the query process, here is a good Uber example:
[Unwinding Uber’s Most Efficient Service](https://medium.com/@buckhx/unwinding-uber-s-most-efficient-service-406413c5871d#.dg5v6irao)
and the [Chinese translation version](https://segmentfault.com/a/1190000008657566)

## Architecture

![](https://user-images.githubusercontent.com/5362577/53225664-bf6abc80-36b3-11e9-8b8e-41611fc7098e.png)
=======
If you are wondering how the spatial index accelerate the query process,
here is a good Uber example: [Unwinding Uber’s Most Efficient
Service](https://medium.com/@buckhx/unwinding-uber-s-most-efficient-service-406413c5871d#.dg5v6irao)
and the [Chinese translation
version](https://segmentfault.com/a/1190000008657566)
